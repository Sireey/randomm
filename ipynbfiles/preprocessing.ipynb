{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ict\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\ict\\.venv\\Lib\\site-packages\\albumentations\\check_version.py:107: UserWarning: Error fetching version info <urlopen error [Errno 11001] getaddrinfo failed>\n",
      "  data = fetch_version_info()\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from indicnlp.tokenize import indic_tokenize\n",
    "import numpy as np\n",
    "import torch\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_telugu_captions(filepath):\n",
    "    captions_dict = {}\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            img_name, caption = parts[0].split(\"#\")[0], parts[1]\n",
    "            if img_name not in captions_dict:\n",
    "                captions_dict[img_name] = []\n",
    "            captions_dict[img_name].append(caption)\n",
    "    return captions_dict\n",
    "\n",
    "# Convert text to Unicode IDs (Simple Tokenizer Workaround)\n",
    "def text_to_ids(text):\n",
    "    tokens = indic_tokenize.trivial_tokenize(text, lang='te')\n",
    "    token_ids = [ord(char) for token in tokens for char in token]  # Unicode ID mapping\n",
    "    return token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indicnlp.tokenize import indic_tokenize\n",
    "\n",
    "class TeluguDataset:\n",
    "    def __init__(self, df, tfms, vocab):\n",
    "        self.df = df\n",
    "        self.tfms = tfms\n",
    "        self.vocab = vocab  # Dictionary mapping words to indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.df.iloc[idx, :]\n",
    "        image_path = sample[\"image\"]\n",
    "        caption = sample[\"caption\"]\n",
    "\n",
    "        # Load and transform image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image = np.array(image)\n",
    "        image = self.tfms(image=image)[\"image\"]\n",
    "\n",
    "        # Process caption using IndicNLP tokenizer\n",
    "        tokens = indic_tokenize.trivial_tokenize(caption, lang='te')  \n",
    "        token_ids = [self.vocab.get(token, self.vocab[\"<unk>\"]) for token in tokens]  \n",
    "\n",
    "        # Add special tokens\n",
    "        token_ids = [self.vocab[\"<s>\"]] + token_ids + [self.vocab[\"</s>\"]]\n",
    "\n",
    "        labels = token_ids.copy()\n",
    "        labels[:-1] = token_ids[1:]\n",
    "\n",
    "        return image, token_ids, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(df, min_freq=2):\n",
    "    word_freq = Counter()\n",
    "\n",
    "    # Tokenize captions and count word occurrences\n",
    "    for caption in df[\"caption\"]:\n",
    "        tokens = indic_tokenize.trivial_tokenize(caption, lang='te')\n",
    "        word_freq.update(tokens)\n",
    "\n",
    "    # Keep only words that appear at least `min_freq` times\n",
    "    vocab_words = [word for word, freq in word_freq.items() if freq >= min_freq]\n",
    "\n",
    "    # Create vocab mapping\n",
    "    vocab = {word: idx for idx, word in enumerate(vocab_words, start=4)}\n",
    "\n",
    "    # Add special tokens\n",
    "    vocab[\"<pad>\"] = 0\n",
    "    vocab[\"<unk>\"] = 1\n",
    "    vocab[\"<s>\"] = 2\n",
    "    vocab[\"</s>\"] = 3\n",
    "\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "telugu_captions = load_telugu_captions(\"D:/ict/Data/fl8telugu.txt\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame([\n",
    "    {\"image\": f\"D:/ict/Data/Images/{img}\", \"caption\": caption} \n",
    "    for img, captions in telugu_captions.items() \n",
    "    for caption in captions\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 8766\n"
     ]
    }
   ],
   "source": [
    "vocab = build_vocab(df, min_freq=2)  # Adjust `min_freq` as needed\n",
    "print(\"Vocabulary Size:\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # Extract images, input_ids, and labels from the batch\n",
    "    image = [i[0] for i in batch]\n",
    "    input_ids = [i[1] for i in batch]\n",
    "    labels = [i[2] for i in batch]\n",
    "\n",
    "    # Stack images into a single tensor (batch dimension)\n",
    "    image = torch.stack(image, dim=0)\n",
    "\n",
    "    # Pad input_ids (captions) and labels to the longest sequence in the batch\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence([torch.tensor(ids) for ids in input_ids], batch_first=True, padding_value=0)\n",
    "    labels = torch.nn.utils.rnn.pad_sequence([torch.tensor(ids) for ids in labels], batch_first=True, padding_value=0)\n",
    "\n",
    "    # Create a mask for real tokens (not padding)\n",
    "    mask = (input_ids != 0).long()\n",
    "\n",
    "    # Set padding tokens in labels to -100 (to ignore during loss calculation)\n",
    "    labels[mask == 0] = -100\n",
    "\n",
    "    return image, input_ids, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ict\\.venv\\Lib\\site-packages\\albumentations\\core\\validation.py:58: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n",
      "C:\\Users\\TechMadmin\\AppData\\Local\\Temp\\ipykernel_11440\\27032829.py:17: UserWarning: Argument(s) 'always_apply' are not valid for transform Normalize\n",
      "  A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], always_apply=True),\n"
     ]
    }
   ],
   "source": [
    "caption_path = \"D:/ict/Data/fl8telugu.txt\"\n",
    "telugu_captions = load_telugu_captions(caption_path)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(\n",
    "    [{\"image\": f\"D:/ict/Data/Images/{img}\", \"caption\": caption} for img, captions in telugu_captions.items() for caption in captions]\n",
    ")\n",
    "\n",
    "# Define Transformations\n",
    "train_tfms = A.Compose([\n",
    "    A.HorizontalFlip(),\n",
    "    A.RandomBrightnessContrast(),\n",
    "    A.ColorJitter(),\n",
    "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.3, rotate_limit=45, p=0.5),\n",
    "    A.HueSaturationValue(p=0.3),\n",
    "    A.Resize(224, 224),\n",
    "    A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], always_apply=True),\n",
    "    ToTensorV2()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Image Shape: torch.Size([3, 224, 224])\n",
      "Sample Token IDs: [2, 4, 5, 6, 7, 8, 1, 9, 10, 11, 12, 13, 14, 3]\n",
      "Sample Labels: [4, 5, 6, 7, 8, 1, 9, 10, 11, 12, 13, 14, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "dataset = TeluguDataset(df, train_tfms, vocab)\n",
    "\n",
    "# Test Preprocessing\n",
    "sample_image, sample_input_ids, sample_labels = dataset[0]\n",
    "\n",
    "print(\"Sample Image Shape:\", sample_image.shape)\n",
    "print(\"Sample Token IDs:\", sample_input_ids)\n",
    "print(\"Sample Labels:\", sample_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded Caption: <s> గులాబీ రంగు దుస్తులు ధరించిన పిల్లవాడు <unk> మార్గంలో ఒక మెట్ల పైకి ఎక్కుతున్నాడు . </s>\n"
     ]
    }
   ],
   "source": [
    "idx_to_word = {idx: word for word, idx in vocab.items()}\n",
    "tokens = [idx_to_word[idx] for idx in sample_input_ids]\n",
    "print(\"Decoded Caption:\", \" \".join(tokens))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
